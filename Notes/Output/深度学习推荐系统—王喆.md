### 深度学习推荐系统 —— 王喆

#### chpt1 互联网的增长引擎——推荐系统

概念：

* UGC(User Generated Content)，典型如Youtube、Tiktok
* CVR (Conversion Rate)、观看时长(Youtube Recommendations)

推荐系统架构：数据与模型

* 物品信息、用户信息、场景信息
* 数据离线批处理、实时流处理



#### chpt2 前深度学习时代——推荐系统的进化之路

1. 演化关系图  p13

2. 协同过滤(CF)

UserCF (1994)
* 用户相似度计算：余弦相似度/皮尔逊相关系数/引入物品平均分
* 最终结果的排序：按相似度加权
* 特点：社交特性更强
* 缺点：
  * 用户太多，在线存储系统存不下矩阵。 
  * 不适用于正反馈获取困难的应用场景（酒店预定、大件商品购买），用户历史数据向量稀疏

UserCF 和 ItemCF 的对比
* UserCF 适合用在个性化需求不强，热点很明显的领域，比如新闻，电影推荐
* ItemCF 适合用在个性化需求比较强，长尾比较长的领域，比如书，电商的推荐

协同过滤的缺点
* 推荐系统的头部效应较明显，处理稀疏向量的能力弱 => MF用更稠密的隐向量表示用户和物品
* 无法引入场景信息和更精细的用户/物品信息 => LR模型、机器学习模型



3. 矩阵分解(MF)

* 用户和物品的隐向量通过分解协同过滤生成的共现矩阵得到
* 矩阵分解的方法：特征值分解（方阵，不适用）；SVD（计算量大、要求稠密）；梯度下降法
* 矩阵分解可加入偏差向量
* 优点：
  * 泛化能力强
  * 空间复杂度低
  * 更好的扩展性和灵活性
* 缺点
  * 同样不方便加入场景信息和更精细的用户/物品信息



4. 逻辑回归 (LR)

将推荐问题转换为一个CTR预估问题

流行原因：1）数学含义上的支撑 2）可解释性强 3）工程化的需要



5. 从FM到FFM——自动特征交叉的解决方案

* Poly2——特征交叉的开始
* FM模型——隐向量特征交叉 (2012-2014)
  * 参数减少到n*k，减小训练开销，增大收敛可能
* FFM模型——引入特征域的概念 (2015)



6. GBDT+LR——特征工程模型化的开端

组合模型，一定程度上解决特征交叉的问题

优点：e2e特征工程模型化

缺点：容易过拟合；丢失大量特征的数值信息



7. LS-PLM —— alibaba曾经的主流推荐模型

也称为MLR(Mix Logistic Regression)，在逻辑回归的基础上加入聚类的思想，其灵感来自对广告推荐领域样本特点的观察

* 超参数：分片数m，阿里的经验值为12

优势：

* 端到端的非线性学习能力
* 模型稀疏性强



引申：

* L1范数比L2范数更容易产生稀疏解，因为加入正则化项的损失函数最小值更容易在参数空间的顶点处取得，对应稀疏参数



#### chpt3 浪潮之巅——深度学习在推荐系统中的应用

1. 演化关系图：p51

2. [AutoRec](https://zhuanlan.zhihu.com/p/159087297) —— 单隐层神经网络推荐模型

结合了auto-encoder和协同过滤的思想，本质上是训练auto-encoder保存推荐系统的泛化信息，输入一个“不完整”的、“真实”的评分条目列表，得到对应这个条目列表的相似条目列表

* I-AutoRec, U-AutoRec

3. Deep Crossing模型 —— 经典的深度学习架构

* 如何解决稀疏特征向量稠密化的问题？
  * 除数值类特征，进入Embedding层
* 如何解决特征自动交叉的问题？
  * 无人工特征交叉，全部交给模型，残差神经网络加强提取非线性特征和组合特征信息的能力
* 如何在输出层中达成问题设定的优化目标？
  * 最后一层逻辑回归

4. NeuralCF模型——CF与深度学习的结合

* 用神经网络替代CF最后一层打分的点积操作
* NeuralCF混合模型：concat原始NeuralCF模型的输出和以element-wise product为互操作的广义矩阵分解模型
* 局限性和CF一致：没引入其他类型的特征、具体的互操作有待探索



5. PNN模型——加强特征交叉能力

Product层：线性操作 + 乘积操作
* 乘积操作 = 内积(IPNN)/外积(OPNN)
* 叠加外积互操作矩阵：本质上是让所有embedding通过一个平均池化层后，再进行外积互操作
  * [embedding的意义](https://www.zhihu.com/question/374835153/answer/1042845667)



6. Wide&Deep模型——记忆能力和泛化能力的综合

具体参考[MLSys.md](https://github.com/huangrt01/CS-Notes/blob/master/Notes/Output/MLSys.md)



7. FM与深度学习模型的结合

* FNN——用FM的隐向量完成Embedding层初始化
  * 背景：Embedding层收敛速度慢 <= 参数多、输入向量稀疏
* DeepFM——用FM代替Wide部分
* NFM——FM的神经网络化尝试
  * 在Embedding层和多层神经网络之间加入**特征交叉池化层**
  * NFM和FM的关系：FM是一种浅层的线性模型, 可以看作是不带隐层的NFM
  * 相比Wide&Deep, Deep&Cross用concat的方法连接特征，NFM方法更侧重特征之间的深度交互



8. 注意力机制在推荐模型中的应用

* AFM——引入注意力机制的FM
  * 在NFM的Pair-wise Interaction Lyaer和池化层之间加入注意力网络
  * 注意力网络 = 全连接层 + softmax
* DIN——引入注意力机制的深度学习网络
  * alibaba的电商广告推荐场景
  * 用户侧的embedding是对每次行为的embedding通过注意力加权得到，注意力权重受广告特征影响
    * 商铺id只和用户历史行为中的商铺id序列发生作用，商品id也如此 <= 注意力轻重更应该由同类信息相关性决定
  *  <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7BV%7D_u%3Df%28%5Ctextbf%7BV%7D_a%29%3D%5Csum_%7Bi%3D1%7D%5EN%5Comega_i%C2%B7%5Ctextbf%7BV%7D_i%3D%5Csum_%7Bi%3D1%7D%5ENg%28%5Ctextbf%7BV%7D_i%2C%5Ctextbf%7BV%7D_a%29%C2%B7%5Ctextbf%7BV%7D_i" alt="\textbf{V}_u=f(\textbf{V}_a)=\sum_{i=1}^N\omega_i·\textbf{V}_i=\sum_{i=1}^Ng(\textbf{V}_i,\textbf{V}_a)·\textbf{V}_i" class="ee_img tr_noresize" eeimg="1">  
  * 注意力激活单元：元素减操作的embedding, concat两个原输入embedding



9. DIEN——序列模型与推荐系统的结合

* 兴趣进化网络
  * 行为序列层：普通的Embedding层
  * 兴趣抽取层：GRU
  * 兴趣进化层：AUGRU，引入注意力机制，与DIN相似，更有针对性地模拟与目标广告相关的兴趣进化路径



10. 强化学习与推荐系统的结合

* 强化学习：“行动—反馈—状态更新”循环
  * DQN
    * 任何深度学习模型都可作为智能体的推荐模型
  * DRN
    * 模型微更新：竞争梯度下降算法（随机扰动、探索网络）
    * 模型主更新：利用历史数据重train
* 意义：变静态为动态，增强模型学习的实时性
  * “重量”与“实时”的折中

#### chpt4 Embedding技术在推荐系统中的应用

1. 什么是Embedding
* 定义：用低维稠密向量“表示”一个对象 
  *	向量“表示”对象特征
  *	向量距离“表示”相似性
  *	具有本体论哲学层面上的意义
* 意义
  * 稀疏特征向量转稠密
  * 本身表达能力强，可引入任何信息进行编码(Graph Embedding)
  * 对物品、用户相似度的计算是常用的推荐系统召回层技术



2. Word2vec——经典的Embedding方法

* CBOW模型和Skip-gram模型
  * 经验上，Skip-gram模型效果更好，输入是本体，输出是周边的词
* 模型训练过程
  * 输入向量的权重矩阵 = 词向量查找表
  * 负采样方法减轻训练负担
    *  <img src="https://www.zhihu.com/equation?tex=E%3D-log%5Csigma%28%7Bv%5E%7B%27%7D_%7Bw_o%7D%7D%5ETh%29-%5Csum_%7Bw_j%5Cin%20W_%7Bneg%7D%7Dlog%5Csigma%28%7Bv%5E%7B%27%7D_%7Bw_j%7D%7D%5ETh%29" alt="E=-log\sigma({v^{'}_{w_o}}^Th)-\sum_{w_j\in W_{neg}}log\sigma({v^{'}_{w_j}}^Th)" class="ee_img tr_noresize" eeimg="1"> 
    * 还有Hierarchical softmax方法加速训练



3. Item2Vec——Word2vec在推荐系统领域的推广



4. Graph Embedding——引入更多结构信息的图嵌入技术



##### Graph Service

* 基于Euler改造
  * 支持全图节点遍历，支持按时间戳采样

* 高性能采样
  * 节点生成前缀和，二分查找随机数
  * 全局点采样：shard有权（shard上所有节点的权重），shard内再次按权
* Graph Embedding on GPU: sample和worker分离
* GE应用
  * 预训练：利用uid与author间的finish关系，构造双向异构图。边权重用finish视频数量，点权重用finish视频总数。
    * 不用gid而用author是为了减小图结构随时间的变化
    * 正负例构造：正例是user及其finish的节点，负例随机采样
  * end2end training
    * 利用uid和cid之间的click关系，边权重是click次数，点权重是click总数
    * 采样：有放回按权采样
    * 优点：训练目标和LTR任务一致
    * 缺点：1.全局图的拟合能力不如业务图 2.end2end结构更自由，可以用node2vec等
  * Finetune BERT/Resnet
    * cid和cid的co-click，边权重用交并比，点权重用click总数，点特征用广告id类泛化特征

#### chpt6 深度学习推荐系统的工程实现

1. 推荐系统的数据流

四种架构：批处理、流计算、Lambda、Kappa

* Lambda: 流计算以增量计算为主，批处理进行全量计算；利用离线层数据对实时流数据进行校验和检错

2. Spark MLlib

3. Parameter Server

* Spark MLlib: 同步阻断式
* Parameter Server: 异步非阻断式

两者区别在于模型参数的分发是否同步

6. 工程与理论之间的均衡

* end2end：强调模型一致性的收益
* two stages：强调模型实时性的收益


##### Potpourri

多样性探索：消重、打散、部分流量未知兴趣